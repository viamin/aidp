# AIDP Configuration File
# This file configures the AI Dev Pipeline harness and providers
# Copy this file to your project root as 'aidp.yml' and customize as needed

# Harness configuration
harness:
  # Maximum number of retries for failed operations
  max_retries: 2

  # Default provider to use when starting execution
  default_provider: "cursor"

  # Fallback providers in order of preference
  fallback_providers: ["claude", "gemini"]

  # Only use providers that don't require API keys
  no_api_keys_required: true

  # Provider weights for load balancing (higher = more preferred)
  provider_weights:
    cursor: 3
    claude: 2
    gemini: 1

  # Advanced harness features
  auto_switch_on_error: true        # Automatically switch providers on errors
  auto_switch_on_rate_limit: true   # Automatically switch providers on rate limits
  max_concurrent_requests: 5        # Maximum concurrent requests per provider
  request_timeout: 300              # Global request timeout in seconds
  user_feedback_timeout: 300        # Timeout for user feedback requests
  work_completion_timeout: 1800     # Timeout for work completion detection

  # Circuit breaker configuration
  circuit_breaker:
    enabled: true
    failure_threshold: 5      # Open circuit after 5 failures
    timeout: 300              # Keep circuit open for 5 minutes
    half_open_max_calls: 3    # Test with 3 calls when half-open

  # Retry configuration
  retry:
    enabled: true
    max_attempts: 3
    base_delay: 1.0           # Initial delay in seconds
    max_delay: 60.0           # Maximum delay in seconds
    exponential_base: 2.0     # Exponential backoff multiplier
    jitter: true              # Add random jitter to delays

  # Rate limiting configuration
  rate_limit:
    enabled: true
    default_reset_time: 3600  # Default reset time in seconds (1 hour)
    burst_limit: 10           # Maximum burst requests
    sustained_limit: 5        # Sustained request limit

  # Load balancing configuration
  load_balancing:
    enabled: true
    strategy: "weighted_round_robin"  # weighted_round_robin, least_connections, random
    health_check_interval: 30        # Health check interval in seconds
    unhealthy_threshold: 3           # Mark unhealthy after 3 failed checks

  # Model switching configuration
  model_switching:
    enabled: true
    auto_switch_on_error: true       # Auto-switch models on errors
    auto_switch_on_rate_limit: true  # Auto-switch models on rate limits
    fallback_strategy: "sequential"  # sequential, load_balanced, random

  # Health check configuration
  health_check:
    enabled: true
    interval: 60              # Health check interval in seconds
    timeout: 10               # Health check timeout in seconds
    failure_threshold: 3      # Mark unhealthy after 3 failures
    success_threshold: 2      # Mark healthy after 2 successes

  # Metrics configuration
  metrics:
    enabled: true
    retention_days: 30        # Keep metrics for 30 days
    aggregation_interval: 300 # Aggregate metrics every 5 minutes
    export_interval: 3600     # Export metrics every hour

  # Session configuration
  session:
    enabled: true
    timeout: 1800             # Session timeout in seconds (30 minutes)
    sticky_sessions: true     # Enable sticky sessions
    session_affinity: "provider_model"  # provider, model, provider_model

  # User interface configuration
  ui:
    enabled: true
    interactive_mode: true    # Enable interactive prompts
    file_selection: true      # Enable @ file selection
    progress_display: true    # Show real-time progress
    status_updates: true      # Show status updates
    pause_resume: true        # Enable pause/resume controls

  # Error handling configuration
  error_handling:
    enabled: true
    log_errors: true          # Log all errors
    retry_on_error: true      # Retry on recoverable errors
    fallback_on_error: true   # Use fallback providers on errors
    error_notification: true  # Notify user of errors

  # Performance optimization
  performance:
    enabled: true
    cache_responses: true     # Cache provider responses
    parallel_processing: true # Enable parallel processing
    batch_requests: true      # Batch multiple requests
    connection_pooling: true  # Use connection pooling

  # Security configuration
  security:
    enabled: true
    ssl_verify: true          # Verify SSL certificates
    allowed_hosts: []         # List of allowed hosts (empty = all)
    blocked_hosts: []         # List of blocked hosts
    api_key_rotation: false   # Enable API key rotation
    audit_logging: true       # Enable audit logging

# Thinking Depth Configuration
# Controls model selection based on task complexity
thinking:
  # Default tier for new requests (mini, standard, thinking, pro, max)
  default_tier: "mini"

  # Maximum tier allowed (prevents excessive costs)
  max_tier: "max"

  # Enable automatic tier escalation on failures
  auto_escalate: true

  # Number of failures before escalating to next tier
  escalation_threshold: 2

  # Per-operation tier overrides (optional)
  overrides:
    # Example: Use mini tier for simple decisions
    # decision.condition_detection: mini
    # decision.error_classification: mini
    # decision.completion_detection: mini

# Zero Framework Cognition (ZFC) Configuration
# Delegates semantic analysis to AI instead of pattern matching
zfc:
  # Master toggle for ZFC features
  enabled: false  # Set to true to enable (experimental)

  # Fallback to legacy pattern matching if AI fails
  fallback_to_legacy: true

  # Individual decision types (can be toggled separately)
  decisions:
    # Condition detection (replaces regex patterns)
    condition_detection:
      enabled: false
      tier: "mini"              # AI tier to use
      cache_ttl: null           # Cache duration in seconds (null = no cache)
      confidence_threshold: 0.7 # Minimum confidence to accept result

    # Error classification (determines if retryable)
    error_classification:
      enabled: false
      tier: "mini"
      cache_ttl: null
      confidence_threshold: 0.7

    # Completion detection (replaces keyword matching)
    completion_detection:
      enabled: false
      tier: "mini"
      cache_ttl: null
      confidence_threshold: 0.8

    # Provider selection (replaces load calculation formula)
    provider_selection:
      enabled: false
      tier: "mini"
      cache_ttl: 300            # Cache for 5 minutes
      confidence_threshold: 0.7

    # Tier escalation decision (replaces heuristic thresholds)
    tier_escalation:
      enabled: false
      tier: "mini"
      cache_ttl: null
      confidence_threshold: 0.7

    # Workflow routing (replaces pattern matching)
    workflow_routing:
      enabled: false
      tier: "mini"
      cache_ttl: 60             # Cache for 1 minute
      confidence_threshold: 0.7

  # Cost controls for ZFC
  cost_limits:
    max_daily_cost: 5.00        # Maximum daily ZFC cost in USD
    max_cost_per_decision: 0.01 # Maximum cost per decision
    alert_threshold: 0.8        # Alert at 80% of budget

  # A/B testing configuration
  ab_testing:
    enabled: false              # Enable A/B testing
    sample_rate: 0.1            # Test 10% of decisions
    log_comparisons: true       # Log ZFC vs legacy comparisons

# Provider configurations
providers:
  # Cursor provider (subscription-based)
  cursor:
    type: "subscription"      # subscription, usage_based, or passthrough
    priority: 1               # Provider priority (higher = more preferred)
    default_flags: []         # Default command-line flags for this provider

    # Available models for this provider
    models: ["cursor-default", "cursor-fast", "cursor-precise"]

    # Model weights for load balancing
    model_weights:
      cursor-default: 3
      cursor-fast: 2
      cursor-precise: 1

    # Model-specific configurations
    models_config:
      cursor-default:
        flags: []
        timeout: 600          # 10 minutes
      cursor-fast:
        flags: ["--fast"]
        timeout: 300          # 5 minutes
      cursor-precise:
        flags: ["--precise"]
        timeout: 900          # 15 minutes

    # Thinking tier model configuration
    # Maps thinking depth tiers to specific models for this provider
    thinking_tiers:
      mini:
        models:
          - cursor-fast       # Fast model for simple tasks
      standard:
        models:
          - cursor-default    # Default model for standard tasks
      thinking:
        models:
          - cursor-precise    # Precise model for complex reasoning
      pro:
        models:
          - cursor-precise    # Use best model for pro tier
      max:
        models:
          - cursor-precise    # Use best model for max tier

    # Provider features
    features:
      file_upload: true
      code_generation: true
      analysis: true

    # Monitoring configuration
    monitoring:
      enabled: true
      metrics_interval: 60    # Metrics collection interval in seconds

    # Rate limiting configuration
    rate_limit:
      enabled: true
      requests_per_minute: 60
      requests_per_hour: 1000
      burst_limit: 10

    # Retry configuration
    retry:
      enabled: true
      max_attempts: 3
      base_delay: 1.0
      max_delay: 60.0
      exponential_base: 2.0
      jitter: true

    # Circuit breaker configuration
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      timeout: 300
      half_open_max_calls: 3

    # Cost tracking
    cost:
      input_cost_per_token: 0.0    # Subscription-based pricing
      output_cost_per_token: 0.0
      fixed_cost_per_request: 0.0
      currency: "USD"

    # Health check configuration
    health_check:
      enabled: true
      interval: 60
      timeout: 10
      failure_threshold: 3
      success_threshold: 2

    # Logging configuration
    log:
      enabled: true
      level: "info"
      file: "logs/cursor.log"
      max_size: 10485760  # 10MB
      max_files: 5
      format: "json"

    # Cache configuration
    cache:
      enabled: true
      ttl: 3600           # 1 hour
      max_size: 100
      strategy: "lru"

    # Security configuration
    security:
      ssl_verify: true
      allowed_hosts: []
      blocked_hosts: []
      timeout: 30
      max_redirects: 5

  # Claude provider (API-based)
  claude:
    type: "usage_based"
    priority: 2
    max_tokens: 100000        # Maximum tokens per request
    default_flags: ["--dangerously-skip-permissions"]

    # Available models for this provider
    models: ["claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022", "claude-3-opus-20240229"]

    # Model weights for load balancing
    model_weights:
      claude-3-5-sonnet-20241022: 3
      claude-3-5-haiku-20241022: 2
      claude-3-opus-20240229: 1

    # Model-specific configurations
    models_config:
      claude-3-5-sonnet-20241022:
        flags: ["--dangerously-skip-permissions"]
        max_tokens: 200000
        timeout: 300          # 5 minutes
      claude-3-5-haiku-20241022:
        flags: ["--dangerously-skip-permissions"]
        max_tokens: 200000
        timeout: 180          # 3 minutes
      claude-3-opus-20240229:
        flags: ["--dangerously-skip-permissions"]
        max_tokens: 200000
        timeout: 600          # 10 minutes

    # Thinking tier model configuration
    # Maps thinking depth tiers to specific models for this provider
    thinking_tiers:
      mini:
        models:
          - claude-3-5-haiku-20241022     # Fastest, cheapest model
      standard:
        models:
          - claude-3-5-sonnet-20241022    # Balanced performance
      thinking:
        models:
          - claude-3-5-sonnet-20241022    # Best balance for complex tasks
      pro:
        models:
          - claude-3-opus-20240229        # Most capable model
      max:
        models:
          - claude-3-opus-20240229        # Maximum capability

    # Authentication configuration
    auth:
      api_key_env: "ANTHROPIC_API_KEY"  # Environment variable for API key

    # Endpoint configuration
    endpoints:
      default: "https://api.anthropic.com/v1/messages"

    # Provider features
    features:
      file_upload: true
      code_generation: true
      analysis: true
      vision: true

    # Monitoring configuration
    monitoring:
      enabled: true
      metrics_interval: 60

  # Gemini provider (API-based)
  gemini:
    type: "usage_based"
    priority: 3
    max_tokens: 50000
    default_flags: []

    # Available models for this provider
    models: ["gemini-1.5-pro", "gemini-1.5-flash", "gemini-1.0-pro"]

    # Model weights for load balancing
    model_weights:
      gemini-1.5-pro: 3
      gemini-1.5-flash: 2
      gemini-1.0-pro: 1

    # Model-specific configurations
    models_config:
      gemini-1.5-pro:
        flags: []
        max_tokens: 100000
        timeout: 300          # 5 minutes
      gemini-1.5-flash:
        flags: []
        max_tokens: 100000
        timeout: 180          # 3 minutes
      gemini-1.0-pro:
        flags: []
        max_tokens: 30000
        timeout: 300          # 5 minutes

    # Thinking tier model configuration
    # Maps thinking depth tiers to specific models for this provider
    thinking_tiers:
      mini:
        models:
          - gemini-1.5-flash      # Fast model for simple tasks
      standard:
        models:
          - gemini-1.5-pro        # Most capable model
      thinking:
        models:
          - gemini-1.5-pro        # Best model for complex reasoning
      pro:
        models:
          - gemini-1.5-pro        # Use best available
      max:
        models:
          - gemini-1.5-pro        # Use best available

    # Authentication configuration
    auth:
      api_key_env: "GEMINI_API_KEY"  # Environment variable for API key

    # Endpoint configuration
    endpoints:
      default: "https://generativelanguage.googleapis.com/v1beta/models"

    # Provider features
    features:
      file_upload: true
      code_generation: true
      analysis: true
      vision: true

    # Monitoring configuration
    monitoring:
      enabled: true
      metrics_interval: 60

    # Rate limiting configuration
    rate_limit:
      enabled: true
      requests_per_minute: 60
      requests_per_hour: 1500
      tokens_per_minute: 32000
      tokens_per_hour: 1000000
      burst_limit: 10

    # Retry configuration
    retry:
      enabled: true
      max_attempts: 3
      base_delay: 1.0
      max_delay: 60.0
      exponential_base: 2.0
      jitter: true
      retry_on_rate_limit: true

    # Circuit breaker configuration
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      timeout: 300
      half_open_max_calls: 3

    # Cost tracking
    cost:
      input_cost_per_token: 0.00000125  # $1.25 per 1M input tokens
      output_cost_per_token: 0.000005   # $5 per 1M output tokens
      fixed_cost_per_request: 0.0
      currency: "USD"

    # Health check configuration
    health_check:
      enabled: true
      interval: 60
      timeout: 10
      failure_threshold: 3
      success_threshold: 2
      check_url: "https://generativelanguage.googleapis.com/v1beta/models"

    # Logging configuration
    log:
      enabled: true
      level: "info"
      file: "logs/gemini.log"
      max_size: 10485760  # 10MB
      max_files: 5
      format: "json"
      log_requests: true
      log_responses: false

    # Cache configuration
    cache:
      enabled: true
      ttl: 1800           # 30 minutes
      max_size: 200
      strategy: "lru"

    # Security configuration
    security:
      ssl_verify: true
      allowed_hosts: ["generativelanguage.googleapis.com"]
      blocked_hosts: []
      timeout: 30
      max_redirects: 5

  # Example passthrough provider
  # openai:
  #   type: "passthrough"
  #   priority: 4
  #   default_flags: ["--model", "gpt-4"]
  #   models: ["gpt-4", "gpt-3.5-turbo"]
  #   model_weights:
  #     gpt-4: 3
  #     gpt-3.5-turbo: 2
  #   models_config:
  #     gpt-4:
  #       flags: ["--model", "gpt-4"]
  #       max_tokens: 128000
  #       timeout: 300
  #     gpt-3.5-turbo:
  #       flags: ["--model", "gpt-3.5-turbo"]
  #       max_tokens: 16000
  #       timeout: 180
  #   auth:
  #     api_key_env: "OPENAI_API_KEY"
  #   endpoints:
  #     default: "https://api.openai.com/v1/chat/completions"
  #   features:
  #     file_upload: true
  #     code_generation: true
  #     analysis: true
  #   monitoring:
  #     enabled: true
  #     metrics_interval: 60

# Provider types explained:
# - "subscription": Uses subscription-based pricing (e.g., Cursor Pro)
# - "usage_based": Uses API-based pricing with token limits
# - "passthrough": Uses underlying service (user manages API keys)

# Environment-specific configurations
environments:
  development:
    harness:
      max_retries: 1
      request_timeout: 60
    providers:
      cursor:
        monitoring:
          enabled: false
        log:
          level: "debug"
      claude:
        rate_limit:
          requests_per_minute: 10
        log:
          level: "debug"

  production:
    harness:
      max_retries: 3
      request_timeout: 300
    providers:
      cursor:
        monitoring:
          enabled: true
        log:
          level: "warn"
      claude:
        rate_limit:
          requests_per_minute: 50
        log:
          level: "info"

# Mode-specific configurations
analyze_mode:
  harness:
    max_retries: 2
    request_timeout: 600
  providers:
    cursor:
      models: ["cursor-default", "cursor-precise"]
    claude:
      models: ["claude-3-5-sonnet-20241022"]

execute_mode:
  harness:
    max_retries: 3
    request_timeout: 300
  providers:
    cursor:
      models: ["cursor-default", "cursor-fast"]
    claude:
      models: ["claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022"]

# Feature flags
features:
  high_performance:
    harness:
      parallel_processing: true
      batch_requests: true
    providers:
      cursor:
        cache:
          ttl: 7200  # 2 hours
      claude:
        cache:
          ttl: 3600  # 1 hour

  debugging:
    harness:
      log_errors: true
      error_notification: true
    providers:
      cursor:
        log:
          level: "debug"
          log_requests: true
          log_responses: true
      claude:
        log:
          level: "debug"
          log_requests: true
          log_responses: true

# Time-based configurations
time_based:
  hours:
    9..17:  # Business hours
      harness:
        max_retries: 2
        request_timeout: 180
      providers:
        claude:
          rate_limit:
            requests_per_minute: 60
    18..8:  # Off hours
      harness:
        max_retries: 1
        request_timeout: 300
      providers:
        claude:
          rate_limit:
            requests_per_minute: 30
  days:
    monday:
      harness:
        max_retries: 3
    friday:
      harness:
        max_retries: 1

# Step-specific configurations
steps:
  analysis:
    harness:
      request_timeout: 600
    providers:
      cursor:
        models: ["cursor-precise"]
      claude:
        models: ["claude-3-5-sonnet-20241022"]
  implementation:
    harness:
      request_timeout: 300
    providers:
      cursor:
        models: ["cursor-default", "cursor-fast"]
      claude:
        models: ["claude-3-5-haiku-20241022"]

# User-specific configurations
users:
  developer1:
    harness:
      max_retries: 2
    providers:
      cursor:
        priority: 1
      claude:
        priority: 2
  developer2:
    harness:
      max_retries: 3
    providers:
      claude:
        priority: 1
      cursor:
        priority: 2

# Devcontainer configuration (experimental)
# Controls behavior when running inside a development container
devcontainer:
  # Enable automatic detection of devcontainer environment
  enabled: true

  # Run agents with full permissions when in devcontainer
  # This allows more dangerous filesystem operations in the sandboxed environment
  # Only applies when AIDP detects it's running in a container
  full_permissions_when_in_devcontainer: false

  # Override detection (useful for testing or forcing behavior)
  # Set to true/false to force detection result, or null for auto-detection
  force_detection: null

  # Permissions to grant when in devcontainer
  permissions:
    # Allow dangerous filesystem operations
    dangerous_filesystem_ops: false

    # Skip permission checks for specific providers
    # Provider-specific flags will be added automatically
    skip_permission_checks:
      - "claude"  # Adds --dangerously-skip-permissions for Claude Code
      # - "cursor"
      # - "gemini"

  # Container-specific settings
  settings:
    # Increase timeouts in container (may be slower)
    timeout_multiplier: 1.5

    # Enable more verbose logging in devcontainer
    verbose_logging: true

    # Network allowlist (for firewall configuration)
    # These will be added to init-firewall.sh if using AIDP's devcontainer
    allowed_domains: []
      # - "api.example.com"
      # - "registry.example.com"

# Work Loop Configuration
# Configures deterministic commands that run during work loops
work_loop:
  enabled: true
  max_iterations: 50
  task_completion_required: true

  # Generic commands array (recommended - replaces category-specific arrays)
  # Each command specifies when it runs: each_unit, full_loop, or on_completion
  commands:
    # Tests run after each work unit iteration
    - name: "unit_tests"
      command: "bundle exec rspec"
      category: "test"
      run_after: "each_unit"      # Run after each agent iteration
      required: true              # Failure blocks completion
      timeout_seconds: 1800

    # Linters run after each work unit iteration
    - name: "lint"
      command: "bundle exec standardrb"
      category: "lint"
      run_after: "each_unit"
      required: true
      timeout_seconds: 300

    # Formatters run only when agent marks work complete
    - name: "format"
      command: "bundle exec standardrb --fix"
      category: "formatter"
      run_after: "on_completion"  # Only run when work is marked complete
      required: false             # Auto-fixers don't block completion

    # Build commands run after each iteration
    - name: "typecheck"
      command: "bundle exec srb tc"
      category: "build"
      run_after: "each_unit"
      required: false             # Optional - warnings don't block

    # Custom commands for project-specific validation
    - name: "security_scan"
      command: "bundle exec brakeman -q"
      category: "custom"
      run_after: "full_loop"      # Only run at end of full work loop
      required: false

  # Legacy category-specific arrays (for backwards compatibility)
  # These are merged with the commands array above
  # test_commands: []
  # lint_commands: []
  # formatter_commands: []
  # build_commands: []
  # documentation_commands: []

  # Output filtering reduces token consumption
  output_filtering:
    enabled: true
    test_mode: "failures_only"    # full, failures_only, minimal
    lint_mode: "failures_only"
    test_max_lines: 500
    lint_max_lines: 300

  # Guards protect sensitive files
  guards:
    enabled: false
    include_files: []
    exclude_files: ["node_modules/**", "dist/**"]
    confirm_files: []             # Files requiring confirmation

# Watch mode configuration
# Configures automated monitoring and processing of GitHub issues/PRs
watch:
  # Label configuration for different triggers
  labels:
    plan_trigger: "aidp-plan"                    # Trigger plan generation
    needs_input: "aidp-needs-input"              # Request clarification
    ready_to_build: "aidp-ready"                 # Ready for implementation
    build_trigger: "aidp-build"                  # Trigger implementation
    review_trigger: "aidp-review"                # Trigger code review
    ci_fix_trigger: "aidp-fix-ci"                # Trigger CI fix
    change_request_trigger: "aidp-request-changes"  # Trigger PR change requests

  # Safety configuration
  safety:
    # Allow watch mode on public repositories (default: false for safety)
    allow_public_repos: false

    # List of GitHub usernames allowed to trigger automated actions
    # Empty list means all authenticated users (for private repos)
    author_allowlist: []
      # - "user1"
      # - "user2"

    # Require running in a container environment (additional safety)
    require_container: false

  # PR change request configuration
  pr_change_requests:
    # Enable/disable PR change request feature
    enabled: true

    # Allow changes across multiple files in a single request
    allow_multi_file_edits: true

    # Run tests and linters before pushing changes
    # If tests fail, changes are committed locally but not pushed
    run_tests_before_push: true

    # Prefix for commit messages created by change request processor
    commit_message_prefix: "aidp: pr-change"

    # Require at least one comment before processing change request
    # This ensures there's context for what changes are requested
    require_comment_reference: true

    # Maximum PR diff size (in lines) to process
    # Large PRs are skipped to avoid overwhelming the AI analysis
    max_diff_size: 2000

# Configuration tips:
# - Set max_tokens based on your API plan limits
# - Use default_flags to customize provider behavior
# - Configure fallback_providers for automatic failover
# - Set no_api_keys_required: true to avoid providers that require API keys
# - Adjust provider_weights to control load balancing
# - Configure model_weights for model selection within providers
# - Set appropriate timeouts for different models
# - Enable monitoring for production environments
# - Use session_affinity for consistent user experience
# - Use environment-specific configs for different deployment stages
# - Use mode-specific configs for different execution modes
# - Use feature flags to enable/disable functionality
# - Use time-based configs for different usage patterns
# - Use step-specific configs for different workflow steps
# - Use user-specific configs for personalized experiences
# - Configure watch mode safety settings before using on public repositories
# - Set author_allowlist to restrict who can trigger automated actions
# - Adjust max_diff_size based on your typical PR sizes and AI capabilities
